
# STEDI Human Balance Analysis â€“ AWS Glue Data Lake Project

## ğŸ“Š Overview

This project uses AWS Glue and other AWS services to build a serverless data lake pipeline for analyzing human balance sensor data from STEDI. The goal is to clean, transform, and prepare the data for future machine learning applications.

---

## ğŸ” Objectives

- Clean and normalize raw data from sensors (Accelerometer, Customer, Step Trainer).
- Upload cleaned data to AWS S3 and catalog with AWS Glue Crawlers.
- Use AWS Glue Jobs (PySpark) to transform and join data for analysis.
- Reduce dataset intelligently to keep only relevant records.
- Document the issues faced and provide solutions.

---

## ğŸ—‚ï¸ Project Structure

```bash
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                  # Raw downloaded data
â”‚   â”œâ”€â”€ cleaned/              # Cleaned and formatted datasets
â”œâ”€â”€ glue-scripts/            # AWS Glue PySpark ETL scripts
â”‚   â”œâ”€â”€ transform_customer.py
â”‚   â”œâ”€â”€ transform_step_trainer.py
â”‚   â””â”€â”€ join_tables.py
â”œâ”€â”€ notebooks/               # Optional Jupyter notebooks for analysis or prototyping
â”‚   â””â”€â”€ exploratory_analysis.ipynb
â”œâ”€â”€ sql/                     # SQL queries used for Athena or debugging
â”‚   â”œâ”€â”€ validate_schema.sql
â”‚   â””â”€â”€ summary_queries.sql
â”œâ”€â”€ src/                     # Custom Python modules or helper scripts
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ s3_loader.py
â”œâ”€â”€ README.md                # Project overview and usage instructions
â””â”€â”€ notes.md                 # Issues encountered and how they were resolved
```

---

## âš™ï¸ Tools & Services

- **AWS Glue** (Crawlers, Jobs, Catalog)
- **AWS S3** (Data Lake storage)
- **AWS Athena** (SQL Querying)
- **PySpark** (Data processing)
- **Jupyter Notebook** (optional for EDA)

---

## ğŸš€ How to Run

1. **Download and clean data**  
   - Fix formatting issues in CSVs.
   - Save cleaned files under `data/cleaned`.

2. **Upload to S3**  
   - Upload to the appropriate S3 bucket: `s3://your-bucket-name/cleaned/`

3. **Run AWS Glue Crawlers**  
   - One for each cleaned dataset to populate AWS Glue Data Catalog.

4. **Run Glue Jobs (ETL)**  
   - Use scripts from the `glue-scripts` directory.

5. **Query with Athena**  
   - Check the transformed data using SQL.

---

## ğŸ§© Issues & Fixes

Documented in [`notes.md`](notes.md) â€” includes:
- Schema mismatches
- Incomplete files
- Crawler and Glue job failures
- Performance tips

---

## ğŸ“ Related Resources

- [STEDI Step Trainer GitHub Repo](#)
- [AWS Glue Documentation](https://docs.aws.amazon.com/glue/index.html)
- [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/)

---

## ğŸ§  Author Notes

> This project builds on previous work but re-implements data transformation using cleaner, more efficient methods. The final goal is to provide an optimized and queryable dataset for balance prediction models.

---

## âœ… Status

âœ”ï¸ Project In Progress  
ğŸ› ï¸ Actively documenting and optimizing transformations  
ğŸ“ˆ Preparing final output for ML model training
